{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "collapsed": true,
        "id": "GnlthgLl9Dkx",
        "outputId": "b111dfa3-5d84-4e5f-8c75-e20b510363ec"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('archive2'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0qtsJhq9Dk0"
      },
      "outputs": [],
      "source": [
        "def load_limited_data_from_directoryt(directory_path, limit):\n",
        "    images = []\n",
        "    labels = []\n",
        "    image_count = 0\n",
        "    for root, dirs, files in os.walk(directory_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".tif\") and \"label\" not in root:\n",
        "                image_path = os.path.join(root, file)\n",
        "                label_path = root.replace(\"train1/images\", \"train1/labels\").replace(\".tif\", \".tif\")\n",
        "                label_path = os.path.join(label_path, file.replace(\".tif\", \".tif\"))  # Append file name to directory path\n",
        "                # print(\"Label path:\", label_path)  # Debugging info\n",
        "                if os.path.exists(label_path):\n",
        "                    try:\n",
        "                        image = io.imread(image_path, as_gray=True)\n",
        "                        image = trans.resize(image, (1024,1024,1))  # Ensure the image has the correct shape\n",
        "                        images.append(image)\n",
        "\n",
        "                        label = io.imread(label_path, as_gray=True)\n",
        "                        label = trans.resize(label, (1024,1024,1))  # Ensure the label has the correct shape\n",
        "                        labels.append(label)\n",
        "\n",
        "                        image_count += 1\n",
        "                        print(\"Train image\", image_count)\n",
        "                        if image_count >= limit:\n",
        "                            break\n",
        "                    except Exception as e:\n",
        "                        print(\"Error loading image or label:\", e)\n",
        "                else:\n",
        "                    print(\"Label path does not exist:\", label_path)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "def load_limited_data_from_directoryv(directory_path, limit):\n",
        "    images = []\n",
        "    labels = []\n",
        "    image_count = 0\n",
        "    for root, dirs, files in os.walk(directory_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".tif\") and \"label\" not in root:\n",
        "                image_path = os.path.join(root, file)\n",
        "                label_path = root.replace(\"val/images\", \"val/labels\").replace(\".tif\", \".tif\")\n",
        "                label_path = os.path.join(label_path, file.replace(\".tif\", \".tif\"))  # Append file name to directory path\n",
        "                # print(\"Label path:\", label_path)  # Debugging info\n",
        "                if os.path.exists(label_path):\n",
        "                    image = io.imread(image_path, as_gray=True)\n",
        "                    image = trans.resize(image, (1024,1024,1))  # Ensure the image has the correct shape\n",
        "                    images.append(image)\n",
        "\n",
        "                    label = io.imread(label_path, as_gray=True)\n",
        "                    label = trans.resize(label, (1024,1024,1))  # Ensure the label has the correct shape\n",
        "                    labels.append(label)\n",
        "\n",
        "                    image_count += 1\n",
        "                    print(\"val image\", image_count)\n",
        "                    if image_count >= limit:\n",
        "                        break\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "def load_limited_data_from_directoryt2(directory_path, limit):\n",
        "    images = []\n",
        "    labels = []\n",
        "    image_count = 0\n",
        "    for root, dirs, files in os.walk(directory_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".tif\") and \"label\" not in root:\n",
        "                image_path = os.path.join(root, file)\n",
        "                label_path = root.replace(\"train2/images\", \"train2/labels\").replace(\".tif\", \".tif\")\n",
        "                label_path = os.path.join(label_path, file.replace(\".tif\", \".tif\"))  # Append file name to directory path\n",
        "                # print(\"Label path:\", label_path)  # Debugging info\n",
        "                if os.path.exists(label_path):\n",
        "                    try:\n",
        "                        image = io.imread(image_path, as_gray=True)\n",
        "                        image = trans.resize(image, (1024,1024,1))  # Ensure the image has the correct shape\n",
        "                        images.append(image)\n",
        "\n",
        "                        label = io.imread(label_path, as_gray=True)\n",
        "                        label = trans.resize(label, (1024,1024,1))  # Ensure the label has the correct shape\n",
        "                        labels.append(label)\n",
        "\n",
        "                        image_count += 1\n",
        "                        print(\"Train image\", image_count)\n",
        "                        if image_count >= limit:\n",
        "                            break\n",
        "                    except Exception as e:\n",
        "                        print(\"Error loading image or label:\", e)\n",
        "                else:\n",
        "                    print(\"Label path does not exist:\", label_path)\n",
        "    return np.array(images), np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gNEMRJfV9Dk1",
        "outputId": "edfdfa9b-90e4-4203-a7d7-218d13f37733"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, concatenate, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Define paths to directories\n",
        "train1_directory_path = 'archive2/train1/images'\n",
        "train2_directory_path = 'archive2/train2/images'\n",
        "val_directory_path = 'archive2/val/images'\n",
        "\n",
        "# Load training and validation data for the first session\n",
        "train_images1, train_labels1 = load_limited_data_from_directoryt(train1_directory_path, limit=1600)\n",
        "val_images1, val_labels1 = load_limited_data_from_directoryv(val_directory_path, limit=800)\n",
        "\n",
        "print(\"Train1 Images Shape:\", train_images1.shape)\n",
        "print(\"Train1 Labels Shape:\", train_labels1.shape)\n",
        "print(\"Validation Images Shape:\", val_images1.shape)\n",
        "print(\"Validation Labels Shape:\", val_labels1.shape)\n",
        "\n",
        "# Show the first training image and label\n",
        "def show_image_and_label(images, labels, index):\n",
        "    if index >= len(images) or index >= len(labels):\n",
        "        print(\"Index out of bounds.\")\n",
        "        return\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axs[0].imshow(images[index].reshape(1024, 1024), cmap='gray')\n",
        "    axs[0].set_title('Image')\n",
        "    axs[1].imshow(labels[index].reshape(1024, 1024), cmap='gray')\n",
        "    axs[1].set_title('Label')\n",
        "    plt.show()\n",
        "\n",
        "show_image_and_label(train_images1, train_labels1, index=50)\n",
        "show_image_and_label(val_images1, val_labels1, index=90)\n",
        "\n",
        "def resnet_model(input_size=(1024, 1024, 1)):\n",
        "    inputs = Input(input_size)\n",
        "    resnet_base = ResNet50(weights=None, include_top=False, input_tensor=inputs)\n",
        "\n",
        "    # Add custom layers for segmentation\n",
        "    x = resnet_base.output\n",
        "    x = Conv2D(256, 3, activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(128, 3, activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(32, 3, activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(16, 3, activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Instantiate the model\n",
        "model = resnet_model(input_size=(1024, 1024, 1))\n",
        "\n",
        "# Load weights from the previously trained model (if any)\n",
        "weights_path = 'resnet_weights.weights.h5'\n",
        "if os.path.exists(weights_path):\n",
        "    model.load_weights(weights_path)\n",
        "    print(\"Weights loaded.\")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model for the first session\n",
        "history1 = model.fit(train_images1, train_labels1, validation_data=(val_images1, val_labels1), epochs=20, batch_size=4)\n",
        "\n",
        "# Save weights after the first training session\n",
        "model.save_weights(weights_path)\n",
        "\n",
        "# Load training and validation data for the second session\n",
        "train_images2, train_labels2 = load_limited_data_from_directoryt2(train2_directory_path, limit=1600)\n",
        "val_images2, val_labels2 = load_limited_data_from_directoryv(val_directory_path, limit=800)\n",
        "\n",
        "print(\"Train2 Images Shape:\", train_images2.shape)\n",
        "print(\"Train2 Labels Shape:\", train_labels2.shape)\n",
        "print(\"Validation Images Shape:\", val_images2.shape)\n",
        "print(\"Validation Labels Shape:\", val_labels2.shape)\n",
        "\n",
        "show_image_and_label(train_images2, train_labels2, index=10)\n",
        "show_image_and_label(val_images2, val_labels2, index=30)\n",
        "\n",
        "# Train the model for the second session\n",
        "history2 = model.fit(train_images2, train_labels2, validation_data=(val_images2, val_labels2), epochs=20, batch_size=4)\n",
        "\n",
        "# Save weights after the second training session\n",
        "model.save_weights(weights_path)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('res_roof_segmentation_model.keras')\n",
        "\n",
        "# Plot combined training and validation loss\n",
        "def plot_combined_loss(history1, history2):\n",
        "    loss = history1.history['loss'] + history2.history['loss']\n",
        "    val_loss = history1.history['val_loss'] + history2.history['val_loss']\n",
        "    epochs = range(1, len(loss) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_combined_loss(history1, history2)\n",
        "\n",
        "# Predict on the validation data\n",
        "val_predictions1 = model.predict(val_images1)\n",
        "val_predictions2 = model.predict(val_images2)\n",
        "\n",
        "# Calculate metrics for both validation sets\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = (y_pred.flatten() > 0.5).astype(int)  # Binarize predictions\n",
        "    y_true = (y_true > 0.5).astype(int)  # Binarize true labels\n",
        "\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    return precision, recall, f1, cm\n",
        "\n",
        "precision1, recall1, f1_1, cm1 = calculate_metrics(val_labels1, val_predictions1)\n",
        "precision2, recall2, f1_2, cm2 = calculate_metrics(val_labels2, val_predictions2)\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "metrics_data = {\n",
        "    'Session': ['Session 1', 'Session 2'],\n",
        "    'Precision': [precision1, precision2],\n",
        "    'Recall': [recall1, recall2],\n",
        "    'F1 Score': [f1_1, f1_2],\n",
        "    'Confusion Matrix': [cm1, cm2]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "print(metrics_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zpZv5Zsq9Dk2",
        "outputId": "8c63fb55-b22a-4493-d3e6-3de446834578"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, concatenate, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Define paths to directories\n",
        "train1_directory_path = 'archive2/train1/images'\n",
        "train2_directory_path = 'archive2/train2/images'\n",
        "val_directory_path = 'archive2/val/images'\n",
        "\n",
        "# Load training and validation data for the first session\n",
        "train_images1, train_labels1 = load_limited_data_from_directoryt(train1_directory_path, limit=1600)\n",
        "val_images1, val_labels1 = load_limited_data_from_directoryv(val_directory_path, limit=800)\n",
        "# Load training and validation data for the second session\n",
        "train_images2, train_labels2 = load_limited_data_from_directoryt2(train2_directory_path, limit=1600)\n",
        "\n",
        "print(\"Train1 Images Shape:\", train_images1.shape)\n",
        "print(\"Train1 Labels Shape:\", train_labels1.shape)\n",
        "print(\"Validation Images Shape:\", val_images1.shape)\n",
        "print(\"Validation Labels Shape:\", val_labels1.shape)\n",
        "print(\"Train2 Images Shape:\", train_images2.shape)\n",
        "print(\"Train2 Labels Shape:\", train_labels2.shape)\n",
        "\n",
        "# Show the first training image and label\n",
        "def show_image_and_label(images, labels, index):\n",
        "    if index >= len(images) or index >= len(labels):\n",
        "        print(\"Index out of bounds.\")\n",
        "        return\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axs[0].imshow(images[index].reshape(1024, 1024), cmap='gray')\n",
        "    axs[0].set_title('Image')\n",
        "    axs[1].imshow(labels[index].reshape(1024, 1024), cmap='gray')\n",
        "    axs[1].set_title('Label')\n",
        "    plt.show()\n",
        "\n",
        "show_image_and_label(train_images1, train_labels1, index=50)\n",
        "show_image_and_label(val_images1, val_labels1, index=90)\n",
        "show_image_and_label(train_images2, train_labels2, index=10)\n",
        "show_image_and_label(val_images1, val_labels1, index=30)\n",
        "show_image_and_label(train_images1, train_labels1, index=20)\n",
        "show_image_and_label(val_images1, val_labels1, index=20)\n",
        "show_image_and_label(train_images2, train_labels2, index=45)\n",
        "show_image_and_label(val_images1, val_labels1, index=45)\n",
        "\n",
        "print(\"Res-Net Starts\")\n",
        "\n",
        "def resnet_model(input_size=(1024, 1024, 1)):\n",
        "    inputs = Input(input_size)\n",
        "    resnet_base = ResNet50(weights=None, include_top=False, input_tensor=inputs)\n",
        "\n",
        "    # Add custom layers for segmentation\n",
        "    x = resnet_base.output\n",
        "    x = Conv2D(256, 3, activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(128, 3, activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(32, 3, activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(16, 3, activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Instantiate the model\n",
        "model = resnet_model(input_size=(1024, 1024, 1))\n",
        "\n",
        "# Load weights from the previously trained model (if any)\n",
        "weights_path = 'resnet_weights.weights.h5'\n",
        "if os.path.exists(weights_path):\n",
        "    model.load_weights(weights_path)\n",
        "    print(\"Weights loaded.\")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model for the first session\n",
        "history1 = model.fit(train_images1, train_labels1, validation_data=(val_images1, val_labels1), epochs=5, batch_size=4)\n",
        "\n",
        "# Save weights after the first training session\n",
        "model.save_weights(weights_path)\n",
        "\n",
        "\n",
        "# Train the model for the second session\n",
        "history2 = model.fit(train_images2, train_labels2, validation_data=(val_images1, val_labels1), epochs=5, batch_size=4)\n",
        "\n",
        "# Save weights after the second training session\n",
        "model.save_weights(weights_path)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('res_roof_segmentation_model.keras')\n",
        "\n",
        "# Plot combined training and validation loss\n",
        "def plot_combined_loss(history1, history2):\n",
        "    loss = history1.history['loss'] + history2.history['loss']\n",
        "    val_loss = history1.history['val_loss'] + history2.history['val_loss']\n",
        "    epochs = range(1, len(loss) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_combined_loss(history1, history2)\n",
        "\n",
        "# Predict on the validation data\n",
        "val_predictions1 = model.predict(val_images1)\n",
        "val_predictions2 = model.predict(val_images1)\n",
        "\n",
        "# Calculate metrics for both validation sets\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = (y_pred.flatten() > 0.5).astype(int)  # Binarize predictions\n",
        "    y_true = (y_true > 0.5).astype(int)  # Binarize true labels\n",
        "\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    return precision, recall, f1, cm\n",
        "\n",
        "precision1, recall1, f1_1, cm1 = calculate_metrics(val_labels1, val_predictions1)\n",
        "precision2, recall2, f1_2, cm2 = calculate_metrics(val_labels1, val_predictions2)\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "metrics_data = {\n",
        "    'Session': ['Session 1', 'Session 2'],\n",
        "    'Precision': [precision1, precision2],\n",
        "    'Recall': [recall1, recall2],\n",
        "    'F1 Score': [f1_1, f1_2],\n",
        "    'Confusion Matrix': [cm1, cm2]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "print(metrics_df)\n",
        "\n",
        "#---------------------------------------------------------------------U-Net---------------------------------------------------------------------------#\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "import numpy as np\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as keras\n",
        "\n",
        "print(\"U-Net Starts\")\n",
        "\n",
        "def unet(pretrained_weights = None,input_size = (1024,1024,1)):\n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "    merge6 = concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs = inputs, outputs = conv10)\n",
        "\n",
        "    model.compile(optimizer = Adam(learning_rate = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    if(pretrained_weights):\n",
        "        model.load_weights(pretrained_weights)\n",
        "\n",
        "    return model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "# Function to display training image and label\n",
        "def show_image_and_label(images, labels, index):\n",
        "    if index >= len(images) or index >= len(labels):\n",
        "        print(\"Index out of bounds.\")\n",
        "        return\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axs[0].imshow(images[index].reshape(1024,1024,1), cmap='gray')\n",
        "    axs[0].set_title('Image')\n",
        "    axs[1].imshow(labels[index].reshape(1024,1024,1), cmap='gray')\n",
        "    axs[1].set_title('Label')\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot combined training and validation loss\n",
        "def plot_combined_loss(history1, history2):\n",
        "    loss = history1.history['loss'] + history2.history['loss']\n",
        "    val_loss = history1.history['val_loss'] + history2.history['val_loss']\n",
        "    epochs = range(1, len(loss) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Function to calculate metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = (y_pred.flatten() > 0.5).astype(int)  # Binarize predictions\n",
        "    y_true = (y_true > 0.5).astype(int)  # Binarize true labels\n",
        "\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    return precision, recall, f1, cm\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "model = unet(input_size=(1024,1024,1))\n",
        "\n",
        "# Load weights from the previously trained model (if any)\n",
        "weights_path = 'unet_weights.weights.h5'\n",
        "if os.path.exists(weights_path):\n",
        "    model.load_weights(weights_path)\n",
        "    print(\"Weights loaded.\")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model for the first session\n",
        "history1 = model.fit(train_images1, train_labels1, validation_data=(val_images1, val_labels1), epochs=5, batch_size=4)\n",
        "\n",
        "# Save weights after the first training session\n",
        "model.save_weights(weights_path)\n",
        "\n",
        "# Train the model for the second session\n",
        "history2 = model.fit(train_images2, train_labels2, validation_data=(val_images1, val_labels1), epochs=5, batch_size=4)\n",
        "\n",
        "\n",
        "# Save weights after the second training session\n",
        "model.save_weights(weights_path)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('unet_roof_segmentation_model.keras')\n",
        "\n",
        "# Plot combined training and validation loss\n",
        "plot_combined_loss(history1, history2)\n",
        "\n",
        "\n",
        "# Predict on the validation data\n",
        "val_predictions1 = model.predict(val_images1)\n",
        "val_predictions2 = model.predict(val_images1)\n",
        "\n",
        "# Calculate metrics for both validation sets\n",
        "precision1, recall1, f1_1, cm1 = calculate_metrics(val_labels1, val_predictions1)\n",
        "precision2, recall2, f1_2, cm2 = calculate_metrics(val_labels1, val_predictions2)\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "metrics_data = {\n",
        "    'Session': ['Session 1', 'Session 2'],\n",
        "    'Precision': [precision1, precision2],\n",
        "    'Recall': [recall1, recall2],\n",
        "    'F1 Score': [f1_1, f1_2],\n",
        "    'Confusion Matrix': [cm1, cm2]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "print(metrics_df)\n",
        "\n",
        "#-------------------------------------------------------------------PSP-Net---------------------------------------------------------------------------#\n",
        "\n",
        "print(\"PSP-Net Starts\")\n",
        "\n",
        "# Define the convolutional block and pyramid pooling module\n",
        "def convolutional_block(input_tensor, filters, block_identifier):\n",
        "    block_name = 'block_' + str(block_identifier) + '_'\n",
        "    filter1, filter2, filter3 = filters\n",
        "    skip_connection = input_tensor\n",
        "\n",
        "    # Block A\n",
        "    input_tensor = Conv2D(filters=filter1, kernel_size=(1, 1), dilation_rate=(1, 1),\n",
        "                                 padding='same', kernel_initializer='he_normal', name=block_name + 'a')(input_tensor)\n",
        "    input_tensor = BatchNormalization(name=block_name + 'batch_norm_a')(input_tensor)\n",
        "    input_tensor = LeakyReLU(alpha=0.2, name=block_name + 'leakyrelu_a')(input_tensor)\n",
        "\n",
        "    # Block B\n",
        "    input_tensor = Conv2D(filters=filter2, kernel_size=(3, 3), dilation_rate=(2, 2),\n",
        "                                 padding='same', kernel_initializer='he_normal', name=block_name + 'b')(input_tensor)\n",
        "    input_tensor = BatchNormalization(name=block_name + 'batch_norm_b')(input_tensor)\n",
        "    input_tensor = LeakyReLU(alpha=0.2, name=block_name + 'leakyrelu_b')(input_tensor)\n",
        "\n",
        "    # Block C\n",
        "    input_tensor = Conv2D(filters=filter3, kernel_size=(1, 1), dilation_rate=(1, 1),\n",
        "                                 padding='same', kernel_initializer='he_normal', name=block_name + 'c')(input_tensor)\n",
        "    input_tensor = BatchNormalization(name=block_name + 'batch_norm_c')(input_tensor)\n",
        "\n",
        "    # Skip convolutional block for residual\n",
        "    skip_connection = Conv2D(filters=filter3, kernel_size=(3, 3), padding='same', name=block_name + 'skip_conv')(skip_connection)\n",
        "    skip_connection = BatchNormalization(name=block_name + 'batch_norm_skip_conv')(skip_connection)\n",
        "\n",
        "    # Block C + Skip Convolution\n",
        "    input_tensor = Add(name=block_name + 'add')([input_tensor, skip_connection])\n",
        "    input_tensor = ReLU(name=block_name + 'relu')(input_tensor)\n",
        "    return input_tensor\n",
        "\n",
        "def base_convolutional_block(input_layer):\n",
        "    base_result = convolutional_block(input_layer, [32, 32, 64], '1')\n",
        "    base_result = convolutional_block(base_result, [64, 64, 128], '2')\n",
        "    base_result = convolutional_block(base_result, [128, 128, 256], '3')\n",
        "    return base_result\n",
        "\n",
        "class PyramidPooling(tf.keras.layers.Layer):\n",
        "    def __init__(self, pool_size, **kwargs):\n",
        "        super(PyramidPooling, self).__init__(**kwargs)\n",
        "        self.pool_size = pool_size\n",
        "        self.conv = tf.keras.layers.Conv2D(256, (1, 1), padding='same', kernel_initializer='he_normal')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        shape = tf.shape(inputs)\n",
        "        height, width = shape[1], shape[2]\n",
        "        pooled = tf.keras.layers.AveragePooling2D(pool_size=self.pool_size)(inputs)\n",
        "        pooled = self.conv(pooled)\n",
        "        return tf.image.resize(pooled, size=(height, width), method='bilinear')\n",
        "\n",
        "def pyramid_pooling_block(input_tensor):\n",
        "    pyramid_block_1 = PyramidPooling(pool_size=(1, 1))(input_tensor)\n",
        "    pyramid_block_2 = PyramidPooling(pool_size=(2, 2))(input_tensor)\n",
        "    pyramid_block_3 = PyramidPooling(pool_size=(3, 3))(input_tensor)\n",
        "    pyramid_block_4 = PyramidPooling(pool_size=(6, 6))(input_tensor)\n",
        "    output_tensor = Concatenate()([input_tensor, pyramid_block_1, pyramid_block_2, pyramid_block_3, pyramid_block_4])\n",
        "    return output_tensor\n",
        "\n",
        "# Define the PSPNet model\n",
        "def PSPNet(input_size=(256, 256, 1)):\n",
        "    input_layer = Input(shape=input_size)\n",
        "    base_layer = base_convolutional_block(input_layer)\n",
        "    pyramid_block = pyramid_pooling_block(base_layer)\n",
        "    output_layer = Conv2D(1, (1, 1), activation='sigmoid')(pyramid_block)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create PSPNet model instance\n",
        "pspnet_model = PSPNet()\n",
        "\n",
        "# Model summary\n",
        "pspnet_model.summary()\n",
        "\n",
        "# Load weights from the previously trained model (if any)\n",
        "weights_path = 'pspnet_weights.weights.h5'\n",
        "if os.path.exists(weights_path):\n",
        "    model.load_weights(weights_path)\n",
        "    print(\"Weights loaded.\")\n",
        "\n",
        "# Train the model using first batch of data\n",
        "history1 = pspnet_model.fit(train_images1, train_labels1, epochs=1, validation_data=(val_images1, val_labels1))\n",
        "\n",
        "# Save weights after the first training session\n",
        "model.save_weights(weights_path)\n",
        "\n",
        "\n",
        "# Train the model using the second batch of data\n",
        "history2 = pspnet_model.fit(train_images2, train_labels2, epochs=1, validation_data=(val_images1, val_labels1))\n",
        "\n",
        "# Save weights after the second training session\n",
        "model.save_weights(weights_path)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('psp_roof_segmentation_model.keras')\n",
        "\n",
        "# Plot combined training and validation loss\n",
        "plot_combined_loss(history1, history2)\n",
        "\n",
        "# Evaluate the model on validation data\n",
        "val_predictions = pspnet_model.predict(val_images1)\n",
        "precision, recall, f1, cm = calculate_metrics(val_labels1, val_predictions)\n",
        "\n",
        "# Print the metrics in a box format\n",
        "print(f\"{'Metric':<10} {'Value':<10}\")\n",
        "print(f\"{'-'*20}\")\n",
        "print(f\"{'Precision':<10} {precision:<10.4f}\")\n",
        "print(f\"{'Recall':<10} {recall:<10.4f}\")\n",
        "print(f\"{'F1 Score':<10} {f1:<10.4f}\")\n",
        "print(f\"{'-'*20}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "g5UzC8kJ9Dk4",
        "outputId": "5f6e58a3-6421-4963-af91-62e6c3c06b27"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tensorflow keras\n",
        "# remember to run this if the comp has been shut down before"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 4883648,
          "sourceId": 8234241,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5045949,
          "sourceId": 8464126,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30715,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
